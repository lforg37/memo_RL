{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Theorical overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Reinforcement learning in a nutshell\n",
    "\n",
    "From Stuart Russell & Peter Norvig [RusNor2010] :\n",
    "> [Reinforcement learning is a branch of Artificial Intelligence] in which we examine how an agent can learn from success and failure, from reward and punishment.\n",
    "\n",
    "From Wikipedia :\n",
    "> Reinforcement learning is an area of machine learning [...] concerned with how software agents ought to *take actions* in an *environment* so as to *maximize some notion of cumulative reward*\n",
    "\n",
    "The aim of RL is thus to develop optimal control agents which would learn to behave optimally by learning from the reward signal obtained when they interact with the environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Agent and the Environment\n",
    "\n",
    "In typical RL applications, the Agent receive *perceptions* from the environment. Those perceptions are pieces of information from the environment internal state. The agent can also *act* on the environment. When it does so, it gets a *reward signal*  for the action taken. Acting on the environment can make its internal state change.\n",
    "\n",
    "The typical Agent-Environment interaction pattern is shown in the picture below.\n",
    "\n",
    "![he Agent-Environment interacting loop](pics/AgentEnvironment.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximizing the cumulative reward\n",
    "\n",
    "The goal of the learner is to act in a way that maximize the expected cumulative reward, *i.e* to maximize the total reward accross the lifetime of the agent which is defined as :\n",
    "\n",
    "$$\\mathbb{E}\\left(\\sum\\limits_{t = 0}^{+\\infty}\\gamma^tr_t\\right)$$\n",
    "\n",
    "where $r_t$ is the reward that the agent received at iteration $t$ and $\\gamma$ a discount factor which allows to tune the balance between long term and short term gain. It is a value between 0 and 1. When $\\gamma$ is close to one, the actions and possibility far in the future ar taken into account. The extreme case, which is having a $\\gamma$ value of 1, comes down to maximizing over the whole lifetime of the agent. At the opposite, a $\\gamma$ value close to 0 doesn't consider long term consequencies of the action. The extreme case, a $\\gamma$ value of 0, comes down to acting greedily with respect to the immediate reward, *i.e* choosing the action which brings the highest immediate reward with no consideration for the future.\n",
    "\n",
    "In practice, even if we are interested with long-term optimal behaviour, the algorithms may require a $\\gamma$ value slightly lower than 1 to converge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the optimisation criterion fixed, there is still many methods available to actualy optimize it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Artificial Neural Networks\n",
    "\n",
    "As all the technics we used in our investigations were either based or at least using artificial neural networks, we present them quickly for the readers who might not know what it is about. Be aware that this is in no way a thorough introduction, it makes some approximation and doesn't cover the wide variety of artifical neurons. \n",
    "\n",
    "### Artificial neurons\n",
    "\n",
    "The standard artificial neurons or units can be seen as mathematical function. A unit is defined by its number of inputs, its *activation function* and its *weights*. If the unit has $n_{\\mathrm{inputs}}$ inputs and the value of the input $i$ is called $x_i$, then it has one weight per input (so the unit as $n_{\\mathrm{inputs}} + 1$ weights as a special input with constant value of 1 which is called the bias is allways present) and the function computed by the unit is the composition of the activation function with a linear combination of the inputs with the unit weights. The expression of the unit output if the activation function of the unit is $f$ is thus\n",
    "\n",
    "$$f\\left(\\sum\\limits_{i\\;=\\;0}^{n_{\\mathrm{inputs}}}w_i\\times x_i\\right)$$\n",
    "\n",
    "### Artificial neural networks\n",
    "An artificial neural networks is a set of units where the outputs of some units are used as the input of some others. A lot of architectures are available. The units which take for input real data are called input units, the ones from which we take the result are called output units and the other which live inside the network and are not directly connected to the exterior are called hidden unit.\n",
    "\n",
    "It is worth to mention that in some architectures like convolutional neural networks, some unit share their weight in order to reduce the number of parameters of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NN can be seen as series of non-linear transformations of linear combinations of the inputs in each layer into which a bias is added.\n",
    "\n",
    "![A fully connected artificial neural network with four inputs, one output and five hidden](pics/fig_neural_network_1.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement learning for video games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "The videogame field offers interesting properties for reinforcement learning algorithm testing. \n",
    "First of all the reward and actions are quite straightforward to determine. Secondly installation is much simpler than setting up for example a robotic test bench. And lastly, the simulation can be launched as many times as wanted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Arcade Learning Environment\n",
    "\n",
    "For our simulation we used the Arcade Learning Environment framework. This framework is widely used in the machine learning community. It offers the possibility to train our agent on half a hundred of atari classic games such as Pac-Man and breakout.\n",
    "\n",
    "The Arcade Learning Environment allows to retrieve the screen pixel values and the score of the game. It does no processing so the inputs are raw inputs and the relevant features have to be extracted by the learner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The genetic approach\n",
    "\n",
    "Our first approach used genetic algorithms to learn a game strategy. Genetic algorithms are a way of exploring a solution space inspired by the evolution theory. The best way to understand the underlying idea is to see them in action on a toy problem. \n",
    "\n",
    "Let's imagine a game that consist in guessing a 6 character word. A solution is rated by the number of letter that are correct.\n",
    "\n",
    "The genetic algorithm will proceed by generating a batch of random candidates. Each candidate is then rated and the candidates of the next generation are produced by combining parts of the candidate of the previous generation. The probability for a given of beeing choosen for contributing to the configuration of a new candidate is proportional to its score. Other bio inspired features such as random mutations are implemented in genetic algorithms. Generation after generation, the score is getting better until a matching solution is found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following python program implements an genetic solution search on the toy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The secret word is REIN and was found in 2 iterations\n",
      "solution found in 0.008064467000167497 s\n",
      "Average number of iteration needed :  2.56\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "import timeit\n",
    "\n",
    "#Change here the secret word. Please take care to choose an uppercase unaccentuated word !\n",
    "secret       = \"REIN\"\n",
    "solutionSize = len(secret)\n",
    "\n",
    "nbCandidates = 200\n",
    "subPopSize   = 40\n",
    "probMutation = 0.05\n",
    "\n",
    "def geneticSearch(printOutput):\n",
    "        candidates = generateCandidates(nbCandidates)\n",
    "        candidates.sort(key = rateCandidate, reverse = True)\n",
    "        time_begin = timeit.default_timer()\n",
    "        nb_iter = 0\n",
    "        best_score   = rateCandidate(candidates[0])\n",
    "        while best_score < solutionSize:\n",
    "            newGeneration = []\n",
    "            candidates = candidates[:subPopSize]\n",
    "            for _ in range(nbCandidates):\n",
    "                p1, p2 = random.sample(candidates, 2)\n",
    "                newGeneration.append(crossoverMutate(p1,p2))\n",
    "            newGeneration.sort(key=rateCandidate, reverse=True)\n",
    "            candidates = newGeneration\n",
    "            #print(\"Iteration \",nb_iter, \" : best word : \",newGeneration[0])\n",
    "            nb_iter += 1\n",
    "            best_score = rateCandidate(newGeneration[0])\n",
    "        time_end = timeit.default_timer()\n",
    "        if printOutput:\n",
    "            print('The secret word is', candidates[0], 'and was found in', nb_iter, 'iterations')\n",
    "            print('solution found in', str(time_end - time_begin), 's')\n",
    "        return nb_iter\n",
    "        \n",
    "    \n",
    "def crossoverMutate(p1, p2):\n",
    "    child = []\n",
    "    rp1 = rateCandidate(p1) + 1\n",
    "    rp2 = rateCandidate(p2) + 1\n",
    "    mixRatio = rp1 / (rp1 + rp2)\n",
    "    for c1, c2 in zip(p1,p2):\n",
    "        if random.random() <= probMutation:\n",
    "            child.append(random.choice(string.ascii_uppercase))\n",
    "        else:\n",
    "            if random.random() <= mixRatio:\n",
    "                child.append(c1)\n",
    "            else:\n",
    "                child.append(c2)\n",
    "    return ''.join(child)\n",
    "    \n",
    "def rateCandidate(candidate):\n",
    "    score = 0\n",
    "    for c1, c2 in zip(candidate, secret):\n",
    "        if c1 == c2:\n",
    "            score += 1\n",
    "    return score\n",
    "\n",
    "def generateCandidates(nbCandidates):\n",
    "    candidates = []\n",
    "    for _ in range(0, nbCandidates):\n",
    "        randomString = ''.join(random.choice(string.ascii_uppercase) for _ in range(solutionSize))\n",
    "        candidates.append(randomString)\n",
    "    return candidates\n",
    "\n",
    "def printCandidates(candidates):\n",
    "    print(' '.join(cand for cand in candidates))\n",
    "\n",
    "geneticSearch(True)\n",
    "\n",
    "nbIters = [geneticSearch(False) for _ in range(25)]\n",
    "\n",
    "print('Average number of iteration needed : ', sum(nbIters)/len(nbIters))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see the performance of the algorithm, let's compare it with a random search with the same number of candidates per generation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The secret word is REIN and was found in 5171 iterations\n",
      "solution found in 9.91076282899985 s\n",
      "Average number of iteration needed :  2152.7\n"
     ]
    }
   ],
   "source": [
    "nb_max_iter = 10000\n",
    "def randomSearch(printOutput):\n",
    "    time_begin = timeit.default_timer()\n",
    "    nb_iter    = 0\n",
    "    best_score = 0\n",
    "    while best_score < solutionSize and nb_iter < nb_max_iter:\n",
    "        candidates   = generateCandidates(nbCandidates)\n",
    "        maxCandidate = max(candidates, key=rateCandidate)\n",
    "        best_score   = rateCandidate(maxCandidate)\n",
    "        nb_iter += 1\n",
    "    \n",
    "    time_end = timeit.default_timer()\n",
    "    if printOutput:\n",
    "        if rateCandidate(maxCandidate) == solutionSize:\n",
    "            print('The secret word is', maxCandidate, 'and was found in', nb_iter, 'iterations')\n",
    "            print('solution found in', str(time_end - time_begin), 's')\n",
    "        else:\n",
    "            print('The solution was not found in', nb_max_iter, 'iterations')\n",
    "    \n",
    "    return nb_iter\n",
    "\n",
    "randomSearch(True)\n",
    "nbIters = [randomSearch(False) for _ in range(10)]\n",
    "\n",
    "print('Average number of iteration needed : ', sum(nbIters)/len(nbIters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genetic evolution of neural networks topologies\n",
    "\n",
    "The NEAT algorithm, introduced in 2002 by K. Stanley and R. Miikkulainen [StaMii2002] is an attempt to use genetic algorithm to determine adequate topology (number of nodes, number of layers, connections between layers) and weights to solve a problem using a neural network.\n",
    "\n",
    "In this approach, the evolving network can be viewed as the internal circuitry of the controler. We do not try to understand it, it is just a way to control the game. \n",
    "\n",
    "To find the good configuration, the NEAT library starts with generating a lot of simple topologies. Those simple topologies are rated on their average final score on the game. Generation after generation, the best topologies are\n",
    "mixed together to create new and more efficient topologies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of NEAT on enginered features\n",
    "\n",
    "As the space search is huge and that those parameters are correlated, the NEAT algorithm isn't efficient on problems with a huge set of inputs. As we however wanted to try this algorithm, we develop our own version of the breakout game in which we had access to a smallest set of intelligent features such as the pad position and the ball position, the brick presence on the screen etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une vidéo de l'agent obtenu est visible dans "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments\n",
    "\n",
    "We try different sets of features and we study the influence of allowing the neural net to have recurrent connection. It appears that a small amount of reccurence tends to diminue the number of generation needed to find a solution.\n",
    "\n",
    "We can note that if we allow the agent to take a lot of time to solve the game, the input set needed to get a solution can be very small and with simple inputs(normalised coordinates of the ball relative to the pad) a solution with as few as 27 unit and 50 links can be found. But as soon as we put a constraint on the time allowed to solve the game, the system complexity grows and a bigger input sets gives better results. The smallest solution which achieve to solve the game in a reasonable time (around human performance) used however around 300 units and 3000 links which is not very efficient for a such little number of inputs.\n",
    "\n",
    "### Conclusions on NEAT\n",
    "\n",
    "NEAT is an interesting approach but it is difficult to use it in a real world problem as it's solution complexity grow rapidly.\n",
    "\n",
    "Research is currently be done to overcome this dificulty and evolutions of NEAT such as HyperNEAT are worth to investigate on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The deep $Q$-Learning approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to $Q$-learning\n",
    "\n",
    "The $Q$-learning method is a reinforcement learning technic based on the so called $Q$ values. The $Q$-value of a state-action pair $(s,a)$ is the maximal expected cumulated (and possibly discounted) reward that an agent can get when it is in state $s$ and takes the action $a$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to create an estimator $\\hat{Q}$ of the $Q$-values of the different state-action pairs. When we have such an estimator, the optimal behaviour is to take at each step the action which has the maximal estimated $Q$-value for the current state $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it is also important to explore the state-action space to improve the quality of the estimator.\n",
    "\n",
    "The simplest way to achieve that is to follow an $\\epsilon$-greedy policy which means that the action is choosen randomly for a proportion $\\epsilon$ of the timesteps and with the above behaviour the rest of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A $Q$-value table can be constructed for small action space problems and filled during the exploration.\n",
    "\n",
    "Let's implement it for the problem of finding the nearest power point on a corridor. \n",
    "\n",
    "The corridor is modeled as an array of 20 cells. There are power point at each end of the corridor.\n",
    "\n",
    "The agent get a reward of -1 for each step and the cells with power point on them are terminal states ($i.e$ which ends the exploration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init S → → ← ← → ← → ← ← ← ← → ← ← → ← → ← S\n",
      "Iteration    0 :  S ← ← ← ← → ← ← ← → ← ← → → → → ← → ← S\n",
      "Iteration    5 :  S ← ← → ← ← → → → → ← ← → → → → → → → S\n",
      "Iteration   10 :  S ← ← → ← ← → → → → ← ← → → → → → → → S\n",
      "Iteration   15 :  S ← ← ← ← ← → → ← → → → → → ← → → → → S\n",
      "Iteration   20 :  S ← ← ← ← ← → → ← ← ← ← → → → → → → → S\n",
      "Iteration   25 :  S ← ← ← ← ← ← ← ← ← ← → → → → → → → → S\n",
      "Iteration   30 :  S ← ← ← ← ← ← ← ← ← ← → → → → → → → → S\n",
      "Iteration   35 :  S ← ← ← ← ← ← ← ← ← ← → → → → → → → → S\n",
      "Iteration   40 :  S ← ← ← ← ← ← ← ← ← ← → → → → → → → → S\n",
      "Iteration   45 :  S ← ← ← ← ← ← ← ← → ← → → → → → → → → S\n",
      "Iteration   50 :  S ← ← ← ← ← ← ← ← → ← → → → → → → → → S\n",
      "Iteration   55 :  S ← ← ← ← ← ← ← ← → ← → → → → → → → → S\n",
      "Iteration   60 :  S ← ← ← ← ← ← ← ← ← → → → → → → → → → S\n",
      "Iteration   65 :  S ← ← ← ← ← ← ← ← ← → → → → → → → → → S\n",
      "Iteration   70 :  S ← ← ← ← ← ← ← ← ← → → → → → → → → → S\n",
      "Iteration   75 :  S ← ← ← ← ← ← ← ← ← → → → → → → → → → S\n",
      "Iteration   80 :  S ← ← ← ← ← ← ← ← ← → → → → → → → → → S\n",
      "Iteration   85 :  S ← ← ← ← ← ← ← ← ← → → → → → → → → → S\n",
      "End S ← ← ← ← ← ← ← ← ← → → → → → → → → → S\n",
      "Estimated V-Value : \n",
      " 0.00 -1.00 -1.95 -2.85 -3.71 -4.52 -5.30 -6.03 -6.73 -7.40 -7.23 -6.73 -6.03 -5.30 -4.52 -3.71 -2.85 -1.95 -1.00 0.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from random import random, choice\n",
    "\n",
    "corridorLength = 20\n",
    "initExplor     = 10\n",
    "epsilon        =  0.05\n",
    "gamma          =  0.95\n",
    "nbAgents       = 201\n",
    "\n",
    "class QTable:\n",
    "    def __init__(self):\n",
    "        self.values = np.random.ranf([corridorLength, 2])\n",
    "        self.values[[0, corridorLength-1]] = 0\n",
    "    \n",
    "    def getAction(self, x):\n",
    "        return np.argmax(self.values[x])\n",
    "    \n",
    "    def getValue(self, x):\n",
    "        return np.max(self.values[x])\n",
    "    \n",
    "    def __str__(self):\n",
    "        chars = ['S']\n",
    "        for i in range(1,corridorLength-1):\n",
    "            if self.values[i][0] > self.values[i][1]:\n",
    "                chars.append('←')\n",
    "            else:\n",
    "                chars.append('→')\n",
    "        chars.append('S')\n",
    "        return \" \".join(chars)\n",
    "    \n",
    "    def getVValues(self):\n",
    "        maximas = np.maximum.reduce(self.values, axis=1)\n",
    "        return \" \".join(\"{0:.2f}\".format(val) for val in maximas)\n",
    "\n",
    "def explore(qtable):\n",
    "    newX = choice(range(1, corridorLength - 1))\n",
    "    while newX == 10:\n",
    "        newX = choice(range(1, corridorLength - 1))\n",
    "    x = newX\n",
    "    while x != 0 and x != corridorLength-1 :\n",
    "        rnd = random()\n",
    "        if rnd < epsilon:\n",
    "            if random() < 0.5:\n",
    "                action = 0\n",
    "            else:\n",
    "                action = 1\n",
    "        else:\n",
    "            action = qtable.getAction(x)\n",
    "\n",
    "        newX = x + 2*action - 1\n",
    "        #update table\n",
    "        qtable.values[x][action] = -1 + gamma * qtable.getValue(newX)\n",
    "        x = newX\n",
    "                \n",
    "    \n",
    "qtable = QTable()\n",
    "print(\"Init\", qtable)\n",
    "i = 0\n",
    "nb_void_iter = 0\n",
    "old_qtable_val = np.copy(qtable.values)\n",
    "\n",
    "while nb_void_iter < 30:\n",
    "    np.copyto(old_qtable_val, qtable.values)\n",
    "    \n",
    "    explore(qtable)\n",
    "    np.subtract(old_qtable_val, qtable.values, old_qtable_val)\n",
    "    if np.max(np.abs(old_qtable_val)) < 0.0001:\n",
    "        nb_void_iter += 1\n",
    "    else:\n",
    "        nb_void_iter = 0\n",
    "    if i%5 == 0:\n",
    "        print(\"Iteration {0:>4d} : \".format(i), qtable)\n",
    "    i+= 1\n",
    "\n",
    "print('End',qtable)\n",
    "print('Estimated V-Value : \\n', qtable.getVValues())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final behaviour is optimal as it will lead the agent to the nearest power point !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimators in large state-action space\n",
    "\n",
    "The naive table based approach doesn't scale well. That's why we need more sophisticated estimators. One Idea, proposed by Mnih and all in [MniKav2015], is to use a neural network to estimate the $Q$ value of the action based on the visual inputs.\n",
    "\n",
    "#### Network architecture\n",
    "The topology preconised in the article is to use a convolutional neural network (CNN) followed by a two layer fully connected feed forward neural net. The CNN is particularily well suited for image analysis, its shared weights helping to detect features independently from localisation in the picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training algorithm\n",
    "\n",
    "The training algorithm for this estimator consists in collecting a labelised dataset from the experiment and using samples from this dataset to adapt the weights of the network. The sampling step is important in order to avoid the high correlation in sequential exemples.\n",
    "\n",
    "The collection of the dataset require two neural networks. The first one play the role of the state value estimator and the second one is used to choose the best action.\n",
    "\n",
    "The exploration cycle follow those steps :\n",
    "\n",
    "1. The agent perceivesthe current state $s$\n",
    "2. The agent chooses an action $a$ based on an $\\epsilon$-greedy policy with respect to the output of the second neural network.\n",
    "3. The agent receives a reward $r$ and perceives the new state $s'$. \n",
    "4. The first neural network is used to estimate the state value $\\hat{V}(s')$ of $s'$ (*id est* the maximal $Q$-value achievable from $s'$)\n",
    "5. The estimated $Q$-value for $(s,a)$ is $r + \\gamma\\hat{V}(s')$\n",
    "\n",
    "All the collected tuple $(s, a, r + \\gamma\\hat{V}(s'))$ are then used as a classical supervised learning case : the weights are ajusted to minimize the error between the second network output for $(s, a)$ and the estimated value $r + \\gamma\\hat{V}(s')$\n",
    "\n",
    "Every $k$ steps ($k$ beeing an hyperparameter), the weights of the second network are copied to the first network so that the estimations of $V$ can improve. This is important for the stability of the model to wait for a certain time before updating the state value estimator network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video controls src=\"videos/Agent.mp4\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributing the exploration phase\n",
    "\n",
    "In [MniBad2016], Mnih and *al* have proposed a way of getting rid of the replay memory by using multiple agents in parallel to decorrelate the sequences of states.\n",
    "\n",
    "We are currently working on implementing those solution with python and the theano library but we are facing some problems concerning shared memory, parallel efficiency and python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "* [MniBad2016] MNIH, Volodymyr, BADIA, Adria Puigdomenech, MIRZA, Mehdi, et al. Asynchronous methods for deep reinforcement learning. arXiv preprint arXiv:1602.01783, 2016.\n",
    "* [MniKav2015] MNIH, Volodymyr, KAVUKCUOGLU, Koray, SILVER, David, et al. Human-level control through deep reinforcement learning. Nature, 2015, vol. 518, no 7540, p. 529-533.\n",
    "* [RusNor2010] RUSSELL S., NORVIG P. Intelligence artificielle. 3e édition. Paris: Pearson education France, 2010, 1199p. ISBN 9782744074554\n",
    "* [StaMii2002] STANLEY, Kenneth O. et MIIKKULAINEN, Risto. Evolving neural networks through augmenting topologies. Evolutionary computation, 2002, vol. 10, no 2, p. 99-127."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
